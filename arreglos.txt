def process_tracking(camera_index, detector, tracker, event_id):
    with app.app_context():
        cap = cv2.VideoCapture(int(camera_index))
        recognized_participants = set()  # Usar un set para evitar reconocimientos duplicados en el mismo ciclo
        try:
            while True:
                ret_val, frame = cap.read()

                if not ret_val:
                    print("Error al leer desde la cámara. Reintentando...")
                    time.sleep(1)
                    continue

                outputs, img_info, bboxes, landmarks = detector.detect_tracking(image=frame)
                if outputs is not None:
                    online_targets = tracker.update(outputs, [img_info["height"], img_info["width"]], (128, 128))
                    tracking_bboxes = []
                    tracking_ids = []

                    # Convertir de [x, y, w, h] a [x_min, y_min, x_max, y_max]
                    for t in online_targets:
                        x, y, w, h = map(int, t.tlwh)
                        tracking_bbox = [x, y, x + w, y + h]
                        tracking_bboxes.append(tracking_bbox)
                        tracking_ids.append(t.track_id)

                    for tid, tbbox in zip(tracking_ids, tracking_bboxes):
                        cv2.rectangle(frame, (tbbox[0], tbbox[1]), (tbbox[2], tbbox[3]), (0, 255, 0), 2)

                        matched_name = "Unknown"
                        highest_score = 0.0  # Inicializa el highest_score para cada cuadro

                        for j, dbbox in enumerate(bboxes):
                            # Convertir dbbox a formato [x_min, y_min, x_max, y_max] si es necesario
                            x_min, y_min, x_max, y_max = map(int, dbbox)
                            dbbox_formatted = [x_min, y_min, x_max, y_max]

                            similarity_score = mapping_bbox(tbbox, dbbox_formatted)
                            print(f"Similitud entre tbbox y dbbox: {similarity_score}")
                            if similarity_score > 0.9:
                                print("iniciado el mapeo")
                                img = img_info["raw_img"]
                                face_alignment = norm_crop(img, landmark=landmarks[j])

                                # Verifica que face_alignment esté bien formado
                                if face_alignment is not None:
                                    print(f"Procesando cara alineada: Dimensiones - {face_alignment.shape}")
                                else:
                                    print("Error: face_alignment es None")
                                    continue

                                # Debug: Verifica las variables antes de reconocimiento
                                print("Antes de recognition: ")
                                print(f"Imagen alineada tipo: {type(face_alignment)}, dimensiones: {face_alignment.shape}")

                                # Llamada a la función de reconocimiento
                                score, name = recognition(face_alignment)

                                # Debug: Muestra lo que devuelve la función recognition
                                print(f"Resultado de recognition -> Score: {score}, Nombre: {name}")

                                if score > highest_score:
                                    highest_score = score
                                    matched_name = name if score > 0.25 else "Unknown"  # Umbral de reconocimiento

                                if matched_name != "Unknown" and matched_name not in recognized_participants:
                                    recognized_participants.add(matched_name)  # Agrega el nombre al set para evitar duplicados

                                    # Inserta el participante en la base de datos si aún no ha sido reconocido
                                    participant = Participant.query.filter_by(ci=matched_name).first()
                                    if participant:
                                        if event_id not in [event.id for event in participant.events]:
                                            event = Event.query.get(event_id)
                                            participant.events.append(event)

                                            attendance = Attendance(event_id=event_id, participant_id=participant.id)
                                            db.session.add(attendance)
                                            db.session.commit()

                        # Mostrar siempre el highest_score, aunque no haya coincidencia de nombre
                        cv2.putText(frame, f'{matched_name} - Score: {highest_score:.2f}', (tbbox[0], tbbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

                # Puedes imprimir el frame aquí para la depuración
                cv2.imshow(f'Camera {camera_index}', frame)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

                time.sleep(0.01)
        finally:
            cap.release()
            cv2.destroyAllWindows()



def process_tracking(camera_index, detector, tracker, event_id):
    with app.app_context():
        cap = cv2.VideoCapture(int(camera_index))
        try:
            while True:
                ret_val, frame = cap.read()

                if not ret_val:
                    print("Error al leer desde la cámara. Reintentando...")
                    time.sleep(1)
                    continue

                # Detección de caras y seguimiento
                outputs, img_info, bboxes, landmarks = detector.detect_tracking(image=frame)
                if outputs is not None:
                    online_targets = tracker.update(outputs, [img_info["height"], img_info["width"]], (128, 128))
                    tracking_bboxes = []
                    tracking_ids = []

                    for t in online_targets:
                        tlwh = t.tlwh  # bounding box en formato [x, y, width, height]
                        tracking_id = t.track_id
                        x, y, w, h = map(int, tlwh)
                        tracking_bboxes.append([x, y, x + w, y + h])
                        tracking_ids.append(tracking_id)

                    for tid, tbbox in zip(tracking_ids, tracking_bboxes):
                        x_min, y_min, x_max, y_max = map(int, tbbox)
                        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)

                        matched_name = "Unknown"
                        highest_score = 0.0

                        for j, dbbox in enumerate(bboxes):
                            similarity_score = mapping_bbox(tbbox, dbbox)
                            print(f"Similitud entre tbbox y dbbox: {similarity_score}")
                            if similarity_score > 0.9:
                                img = img_info["raw_img"]
                                face_alignment = norm_crop(img, landmark=landmarks[j])

                                if face_alignment is not None:
                                    print(f"Procesando cara alineada: Dimensiones - {face_alignment.shape}")
                                else:
                                    print("Error: face_alignment es None")
                                    continue

                                print("Antes de recognition: ")
                                print(f"Imagen alineada tipo: {type(face_alignment)}, dimensiones: {face_alignment.shape}")

                                score, name = recognition(face_alignment)

                                print(f"Resultado de recognition -> Score: {score}, Nombre: {name}")

                                if score > highest_score:
                                    highest_score = score
                                    matched_name = name if score > 0.25 else "Unknown"

                                if matched_name != "Unknown" and score > 0.25:
                                    recognized_participants.append(name)

                                    participant = Participant.query.filter_by(ci=name).first()
                                    if participant:
                                        if event_id not in [event.id for event in participant.events]:
                                            event = Event.query.get(event_id)
                                            participant.events.append(event)

                                            attendance = Attendance(event_id=event_id, participant_id=participant.id)
                                            db.session.add(attendance)
                                            db.session.commit()

                        cv2.putText(frame, f'{matched_name} - Score: {highest_score:.2f}', (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

                cv2.imshow(f'Camera {camera_index}', frame)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

                time.sleep(0.01)
        finally:
            cap.release()
            cv2.destroyAllWindows()


